{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKjxJKf3vtrb"
   },
   "source": [
    "# NLP Answers\n",
    "\n",
    "- **Answer Set**: No. 06\n",
    "- **Full Name**: Mohammad Hosein Nemati\n",
    "- **Student Code**: `610300185`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NokCmrCkvtrk"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCrlSoh4vtrl"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this problem, we are going to use **ParsBERT** pre-trained model for **Token Classification (NER)** task on **Custom** dataset.  \n",
    "Then we will compare the reported metrics to previously trained models in `exercise 3`\n",
    "\n",
    "In the first step, we will import some useful libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8d1e4ab7"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import math as math\n",
    "import hazm as hazm\n",
    "import nltk as nltk\n",
    "import nltk.corpus.reader.conll as nltkconll\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.preprocessing as skprocessing\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6595aa20"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4ed694d"
   },
   "source": [
    "## Loading\n",
    "\n",
    "First of all, we must load our dataset and then shuffle the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "paMlIDu6vtrr",
    "outputId": "f8f8ab33-b72b-494d-f482-201ada2b757d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>[مثالهاي, بالا, از, سميعيان, است, .]</td>\n",
       "      <td>[N, ADJ, P, N, V, DELM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861</th>\n",
       "      <td>[او, \", راو, شانكار, \", هندي, بود, كه, سهتار, ...</td>\n",
       "      <td>[PRO, DELM, N, N, DELM, ADJ, V, CON, N, V, DELM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>[در, واژگان, تنها, اطلاعات, منحصربه, هر, مدخل,...</td>\n",
       "      <td>[P, N, ADV, N, ADJ, QUA, N, ADJ, ADJ, V, CON, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6015</th>\n",
       "      <td>[مثلاً, فردي, را, در, نظر, بگيريد, كه, صبح, از...</td>\n",
       "      <td>[CON, N, P, P, N, V, CON, N, P, N, V, DELM, P,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>[بهطور, سرمايهگذاران, به, دنبال, يك, محيط, تجا...</td>\n",
       "      <td>[CON, N, P, N, N, N, ADJ, ADJ, P, N, V, V, CON...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4122</th>\n",
       "      <td>[اين, كه, شدني, نيست, ،, يعني, نميتوانيم, بگوي...</td>\n",
       "      <td>[PRO, CON, ADJ, V, DELM, CON, V, V, DET, N, N,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2632</th>\n",
       "      <td>[2, -, الكتريكي, .]</td>\n",
       "      <td>[N, DELM, ADJ, DELM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6183</th>\n",
       "      <td>[ساوال, :, شكاف, لب, و, كام, يك, بيماري, مادرز...</td>\n",
       "      <td>[N, DELM, N, N, CON, N, N, N, ADJ, V, DELM, AD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6570</th>\n",
       "      <td>[در, دوره, مظفرالدينشاه, هر, كدام, از, محلههاي...</td>\n",
       "      <td>[P, N, N, QUA, N, P, N, ADJ, N, P, SPEC, N, CO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3592</th>\n",
       "      <td>[به, نظر, ميرسد, كه, كودك, در, هر, مرحله, از, ...</td>\n",
       "      <td>[P, N, V, CON, N, P, QUA, N, P, N, ADJ, PRO, P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8722 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  words  \\\n",
       "64                 [مثالهاي, بالا, از, سميعيان, است, .]   \n",
       "4861  [او, \", راو, شانكار, \", هندي, بود, كه, سهتار, ...   \n",
       "99    [در, واژگان, تنها, اطلاعات, منحصربه, هر, مدخل,...   \n",
       "6015  [مثلاً, فردي, را, در, نظر, بگيريد, كه, صبح, از...   \n",
       "1396  [بهطور, سرمايهگذاران, به, دنبال, يك, محيط, تجا...   \n",
       "...                                                 ...   \n",
       "4122  [اين, كه, شدني, نيست, ،, يعني, نميتوانيم, بگوي...   \n",
       "2632                                [2, -, الكتريكي, .]   \n",
       "6183  [ساوال, :, شكاف, لب, و, كام, يك, بيماري, مادرز...   \n",
       "6570  [در, دوره, مظفرالدينشاه, هر, كدام, از, محلههاي...   \n",
       "3592  [به, نظر, ميرسد, كه, كودك, در, هر, مرحله, از, ...   \n",
       "\n",
       "                                                   tags  \n",
       "64                              [N, ADJ, P, N, V, DELM]  \n",
       "4861   [PRO, DELM, N, N, DELM, ADJ, V, CON, N, V, DELM]  \n",
       "99    [P, N, ADV, N, ADJ, QUA, N, ADJ, ADJ, V, CON, ...  \n",
       "6015  [CON, N, P, P, N, V, CON, N, P, N, V, DELM, P,...  \n",
       "1396  [CON, N, P, N, N, N, ADJ, ADJ, P, N, V, V, CON...  \n",
       "...                                                 ...  \n",
       "4122  [PRO, CON, ADJ, V, DELM, CON, V, V, DET, N, N,...  \n",
       "2632                               [N, DELM, ADJ, DELM]  \n",
       "6183  [N, DELM, N, N, CON, N, N, N, ADJ, V, DELM, AD...  \n",
       "6570  [P, N, N, QUA, N, P, N, ADJ, N, P, SPEC, N, CO...  \n",
       "3592  [P, N, V, CON, N, P, QUA, N, P, N, ADJ, PRO, P...  \n",
       "\n",
       "[8722 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reader = nltkconll.ConllCorpusReader(\"../lib\", [\"Train.txt\"], (\"words\", \"pos\"))\n",
    "test_reader = nltkconll.ConllCorpusReader(\"../lib\", [\"Test.txt\"], (\"words\", \"pos\"))\n",
    "\n",
    "def rows(reader):\n",
    "    for sent in reader.tagged_sents():\n",
    "        words, tags = zip(*sent)\n",
    "        yield [list(words), list(tags)]\n",
    "\n",
    "train_frame = pd.DataFrame(rows(train_reader), columns=[\"words\", \"tags\"])\n",
    "train_frame = train_frame.sample(frac=1, random_state=313)\n",
    "\n",
    "test_frame = pd.DataFrame(rows(test_reader), columns=[\"words\", \"tags\"])\n",
    "test_frame = train_frame.sample(frac=1, random_state=313)\n",
    "\n",
    "train_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rI2REVIevtrv"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHBfyoFSvtrw"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "aG7Q131wvtrx"
   },
   "outputs": [],
   "source": [
    "def tags(reader):\n",
    "    for sent in reader.tagged_sents():\n",
    "        for token in sent:\n",
    "            yield token[1]\n",
    "            \n",
    "tag_encoder = skprocessing.LabelEncoder().fit(list(tags(train_reader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frame[\"tags\"] = train_frame[\"tags\"].apply(lambda tags : tag_encoder.transform(tags))\n",
    "test_frame[\"tags\"] = test_frame[\"tags\"].apply(lambda tags : tag_encoder.transform(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af4a101a75124a9a9e5bfd8f4480c52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd0698d5939a48169729990e107a2f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "def tokenize(ds):\n",
    "    tokens = tokenizer(ds[\"words\"], truncation=True, padding=True, max_length=512, is_split_into_words=True)\n",
    "    labels = []\n",
    "\n",
    "    for i, label in enumerate(ds[\"tags\"]):\n",
    "        word_ids = tokens.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokens[\"labels\"] = labels\n",
    "    return tokens\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-fa-base-uncased\")\n",
    "\n",
    "trainset = Dataset.from_pandas(train_frame).map(tokenize, batched=True)\n",
    "testset = Dataset.from_pandas(test_frame).map(tokenize, batched=True)\n",
    "\n",
    "collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cowVS3e5vtry"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-8nVpIzvtrz"
   },
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "fS1yFaQovtrz"
   },
   "outputs": [],
   "source": [
    "train_set = trainset.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    batch_size=8,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "\n",
    "test_set = testset.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    batch_size=8,\n",
    "    collate_fn=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cowVS3e5vtry"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZjZe48zvtr1"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "J7VFKYmOvtr1",
    "outputId": "64a5220e-3c21-4ed3-c372-0c9fcf134b6e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at HooshvareLab/bert-fa-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForTokenClassification\n",
    "\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(\"HooshvareLab/bert-fa-base-uncased\", num_labels=len(tag_encoder.classes_))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "mUdATQ7rvtr1",
    "outputId": "1f0b0221-612f-4918-f967-721da3134176"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1100/1100 [==============================] - 1132s 1s/step - loss: 0.7946 - accuracy: 0.7883\n",
      "Epoch 2/4\n",
      "1100/1100 [==============================] - 1127s 1s/step - loss: 0.2830 - accuracy: 0.9215\n",
      "Epoch 3/4\n",
      "1100/1100 [==============================] - 1127s 1s/step - loss: 0.1694 - accuracy: 0.9545\n",
      "Epoch 4/4\n",
      "1100/1100 [==============================] - 1126s 1s/step - loss: 0.1043 - accuracy: 0.9723\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc5d461b550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.fit(train_set, epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PO7tU1Xwvtr2"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IVeWlHPvtr2"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "SchKxEX3vtr2",
    "outputId": "7e58b0e0-c21c-4d46-d9e4-ac4d64122129"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\KoLiBer\\Documents\\Workspace\\nlpexercises\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1557, in test_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\KoLiBer\\Documents\\Workspace\\nlpexercises\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1546, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\KoLiBer\\Documents\\Workspace\\nlpexercises\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1535, in run_step  **\n        outputs = model.test_step(data)\n    File \"c:\\Users\\KoLiBer\\Documents\\Workspace\\nlpexercises\\.venv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1492, in test_step\n        y = {key: val for key, val in x.items() if key in label_kwargs}\n\n    AttributeError: 'Tensor' object has no attribute 'items'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\KoLiBer\\Documents\\Workspace\\nlpexercises\\src\\exercise6.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/KoLiBer/Documents/Workspace/nlpexercises/src/exercise6.ipynb#ch0000025?line=0'>1</a>\u001b[0m result \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mevaluate([\u001b[39m1\u001b[39;49m,\u001b[39m2\u001b[39;49m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/KoLiBer/Documents/Workspace/nlpexercises/src/exercise6.ipynb#ch0000025?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(result)\n",
      "File \u001b[1;32mc:\\Users\\KoLiBer\\Documents\\Workspace\\nlpexercises\\.venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/KoLiBer/Documents/Workspace/nlpexercises/.venv/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/KoLiBer/Documents/Workspace/nlpexercises/.venv/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> <a href='file:///c%3A/Users/KoLiBer/Documents/Workspace/nlpexercises/.venv/lib/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/KoLiBer/Documents/Workspace/nlpexercises/.venv/lib/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/KoLiBer/Documents/Workspace/nlpexercises/.venv/lib/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileccefrssq.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/KoLiBer/AppData/Local/Temp/__autograph_generated_fileccefrssq.py?line=12'>13</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/KoLiBer/AppData/Local/Temp/__autograph_generated_fileccefrssq.py?line=13'>14</a>\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/KoLiBer/AppData/Local/Temp/__autograph_generated_fileccefrssq.py?line=14'>15</a>\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     <a href='file:///c%3A/Users/KoLiBer/AppData/Local/Temp/__autograph_generated_fileccefrssq.py?line=15'>16</a>\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/KoLiBer/AppData/Local/Temp/__autograph_generated_fileccefrssq.py?line=16'>17</a>\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KoLiBer\\Documents\\Workspace\\nlpexercises\\.venv\\lib\\site-packages\\transformers\\modeling_tf_utils.py:1492\u001b[0m, in \u001b[0;36mTFPreTrainedModel.test_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/KoLiBer/Documents/Workspace/nlpexercises/.venv/lib/site-packages/transformers/modeling_tf_utils.py?line=1489'>1490</a>\u001b[0m                 x[output_to_label[key]] \u001b[39m=\u001b[39m val\n\u001b[0;32m   <a href='file:///c%3A/Users/KoLiBer/Documents/Workspace/nlpexercises/.venv/lib/site-packages/transformers/modeling_tf_utils.py?line=1490'>1491</a>\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/KoLiBer/Documents/Workspace/nlpexercises/.venv/lib/site-packages/transformers/modeling_tf_utils.py?line=1491'>1492</a>\u001b[0m     y \u001b[39m=\u001b[39m {key: val \u001b[39mfor\u001b[39;00m key, val \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39;49mitems() \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m label_kwargs}\n\u001b[0;32m   <a href='file:///c%3A/Users/KoLiBer/Documents/Workspace/nlpexercises/.venv/lib/site-packages/transformers/modeling_tf_utils.py?line=1492'>1493</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m y \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_using_dummy_loss:\n\u001b[0;32m   <a href='file:///c%3A/Users/KoLiBer/Documents/Workspace/nlpexercises/.venv/lib/site-packages/transformers/modeling_tf_utils.py?line=1493'>1494</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCould not find label column(s) in input dict and no separate labels were provided!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: in user code:\n\n    File \"c:\\Users\\KoLiBer\\Documents\\Workspace\\nlpexercises\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1557, in test_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\KoLiBer\\Documents\\Workspace\\nlpexercises\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1546, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\KoLiBer\\Documents\\Workspace\\nlpexercises\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1535, in run_step  **\n        outputs = model.test_step(data)\n    File \"c:\\Users\\KoLiBer\\Documents\\Workspace\\nlpexercises\\.venv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1492, in test_step\n        y = {key: val for key, val in x.items() if key in label_kwargs}\n\n    AttributeError: 'Tensor' object has no attribute 'items'\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate([1,2])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mCfAcKlvtr3"
   },
   "source": [
    "As we can see, the **Pars BERT** pre-trained model returend much better accuracy after fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QU3mpEmlvtr3"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9278c29f1c5526d5663cab0214876a9a1ad235f0a8fdeebad627cd7666833d94"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
